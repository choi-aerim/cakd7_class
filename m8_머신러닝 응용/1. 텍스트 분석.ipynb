{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33de5827",
   "metadata": {},
   "source": [
    "# 텍스트분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc06bf",
   "metadata": {},
   "source": [
    "## 클렌징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44058de",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfa392",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ceed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp38-cp38-win_amd64.whl (267 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: click in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.9.13 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6197c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# 마침표, 개행문자 등 데이터 세트 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sent_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 문장 개수:3\n",
      "\n",
      " 출력: \n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "\n",
    "print(f' sent_tokenize 객체 타입: \\n{type(sentences)},\\n\\n 문장 개수:{len(sentences)}\\n')\n",
    "print(f' 출력: \\n{sentences}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab04f76",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "- 문장을 단어로 토큰화\n",
    "- 공백, 쉼포, 마침표, 개행문자 등으로 단어를 분리 혹은 정규표현식을 이용해 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de0b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 단어 개수:15\n",
      "\n",
      " 출력: \n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = sentences[0]\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(f' word_tokenize 객체 타입: \\n{type(words)},\\n\\n 단어 개수:{len(words)}\\n')\n",
    "print(f' 출력: \\n{words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e5355",
   "metadata": {},
   "source": [
    "### 문장 및 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "794a5fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 단어 개수:3\n",
      "\n",
      " 출력:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Matrix',\n",
       "  'is',\n",
       "  'everywhere',\n",
       "  'its',\n",
       "  'all',\n",
       "  'around',\n",
       "  'us',\n",
       "  ',',\n",
       "  'here',\n",
       "  'even',\n",
       "  'in',\n",
       "  'this',\n",
       "  'room',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'can',\n",
       "  'see',\n",
       "  'it',\n",
       "  'out',\n",
       "  'your',\n",
       "  'window',\n",
       "  'or',\n",
       "  'on',\n",
       "  'your',\n",
       "  'television',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'feel',\n",
       "  'it',\n",
       "  'when',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  'work',\n",
       "  ',',\n",
       "  'or',\n",
       "  'go',\n",
       "  'to',\n",
       "  'church',\n",
       "  'or',\n",
       "  'pay',\n",
       "  'your',\n",
       "  'taxes',\n",
       "  '.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하도록 만드는 사용자 함수 생성\n",
    "def tokenize_text(text):\n",
    "    ## 문장 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    ## 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "# 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(f' word_tokenize 객체 타입: \\n{type(word_tokens)},\\n\\n 단어 개수:{len(word_tokens)}\\n')\n",
    "print(f' 출력:')\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a4453",
   "metadata": {},
   "source": [
    "### n_gram\n",
    "- 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec783b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "916081bf",
   "metadata": {},
   "source": [
    "## 스톱워드 제거\n",
    "- 문장을 구성하는 필수 문법 요소지만 문맥적으로 큰 의미가 없는 단어를 사전에 제거\n",
    "- 빈번하게 텍스트에 나타나면 중요 단어로 인지될 수 있기 때문에 중요한 작업임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bb89b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 스톱워드 목록 다운로드\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19c5ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스톱워드 리스트 내 단어 개수:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('스톱워드 리스트 내 단어 개수:')\n",
    "len(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9597555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스톱워드 리스트:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('스톱워드 리스트:')\n",
    "nltk.corpus.stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89a63db1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'],\n",
       " ['see', 'window', 'television', '.'],\n",
       " ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 스톱워드 리스트 생성\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    \n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        # 소문자로 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱워드 리스트에 포함되지 않으면 리스트에 담기\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "        # 문장별 걸러진 단어를 최종 리스트에 담기    \n",
    "    all_tokens.append(filtered_words)\n",
    "        \n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0af948",
   "metadata": {},
   "source": [
    "## Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d6157",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- 원형 단어로 변환 시, 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
    "- 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098538f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant faciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Stemmer 객체 생성\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "# stem('단어') 메서드를 활용해 원하는 단어의 stemming 수행\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('faciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff336485",
   "metadata": {},
   "source": [
    "- work의 경우 working, works, worked 등 기본 단어에 ing, s, ed가 붙는 단순 형태라서 제대로 인식함\n",
    "- 하지만, 다른 단의 경우 비교형, 최상급형에서 원형을 정확히 찾지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5f282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab9b8f45",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- 품사와 같은 문법적 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아줌\n",
    "- 따라서 수행시간이 Stemming보다 더 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731b6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607da5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fccec003",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6ccb3",
   "metadata": {},
   "source": [
    "## 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc73c8a",
   "metadata": {},
   "source": [
    "### BOW벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6d36b",
   "metadata": {},
   "source": [
    "### 희소행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c76829",
   "metadata": {},
   "source": [
    "#### 희소행렬 형태의 BOW벡터화한 데이터프레임의 예시를 배열로 만들어서 COO방식으로 저장되는 것 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16425bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 예시 배열 만들기\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da47f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# data의 행의 위치와 열의 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 COO형식 희소행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45006ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66caf5e5",
   "metadata": {},
   "source": [
    "### 희소행렬 - CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19cf443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2 =  np.array([[0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0],[2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32198f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]] \n",
      "\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray(), '\\n')\n",
    "\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5545d5c",
   "metadata": {},
   "source": [
    "#### 과제1: 10행 10열의 희소행렬 (0의 비중 70% 이상)을 생성한 후 COO, CSR 방식으로 변환 후 다시 희소 행렬 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c1526ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1, 0, 2, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a6dac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 1 0 2 0]]\n",
      "[3 0 1 0 2 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(d)\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mindex(d))\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "x = dense.reshape(1,-1)\n",
    "print(x)\n",
    "for d in x:\n",
    "    print(d)\n",
    "    if d != 0:\n",
    "        print(x.index(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(dense):\n",
    "    one_dense = dense.reshape(1,-1)\n",
    "    data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe4f15",
   "metadata": {},
   "source": [
    "# 실습: 텍스트 분류- 20 뉴스 그룹 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d83dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset = 'all', random_state = 156)\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af160112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도\n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "target 클래스의 이름들 \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도\\n', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('\\n')\n",
    "print('target 클래스의 이름들 \\n', news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a32ff5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c7954e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: 11314, 테스트 데이터 크기: 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset = 'train'으로 학습용 데이터만 추출, remove = ('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news = fetch_20newsgroups(subset = 'train',\n",
    "                                remove = ('headers', 'footers', 'quotes'),\n",
    "                                random_state = 156)\n",
    "\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "\n",
    "# subset = 'train'으로 학습용 데이터만 추출, remove = ('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news = fetch_20newsgroups(subset = 'test',\n",
    "                               remove = ('headers', 'footers', 'quotes'),\n",
    "                               random_state = 156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "\n",
    "print(f'학습 데이터 크기: {len(train_news.data)}, 테스트 데이터 크기: {len(test_news.data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad29d4",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀모델 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7181fd3",
   "metadata": {},
   "source": [
    "### CountVectorizer 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "90934653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 텍스트의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorization으로 피처 벡터화 변환 수행\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "\n",
    "# 학습 데이터로 fit() 된 CountVectorizer를 이용해 테스트 데이터를 피처 벡처화 변환 수행\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "\n",
    "print('학습 데이터 텍스트의 CountVectorizer Shape:', X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "459a4156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "## 학습\n",
    "### 주의: 벡터화 한 x_train 데이터 세트 이용해야 함\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "\n",
    "## 예측\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(f'CountVectorized Logistic Regression의 예측 정확도는 {np.round(accuracy,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2497d",
   "metadata": {},
   "source": [
    "### TF-IDF 기반"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951b65",
   "metadata": {},
   "source": [
    "#### 기본 파라미터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d6132d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터화를 적용해 학습데이터 테스트 데이터세트 변환\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF 벡터화를 적용#### 파라미터 수정 후 TF-IDF기반 로지스틱 회귀 모델 적용\n",
    "tfidf_vect.fit(X_train)\n",
    "\n",
    "#X_train, X_test 데이터를 TF-IDF 벡터로 변형시키기\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "\n",
    "# LogisticRegression을 이용해 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "## 학습\n",
    "### 주의: 벡터화한 x_train 데이터 세트 이용\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "\n",
    "## 예측\n",
    "lr_clf.predict(X_test_tfidf_vect)\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(f'TF-IDF Logistic Regression의 예측 정확도는 {np.round(accuracy,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f683e",
   "metadata": {},
   "source": [
    "#### 파라미터 수정 후 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0e84fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파라미터를 다르게 한 TF-IDF Logisitc Regression의 예측 정확도는 0.69\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 수정한 TF-IDF 객체 생성\n",
    "tfidf_vect = TfidfVectorizer(stop_words = 'english', \n",
    "                             ngram_range = (1,2),\n",
    "                             max_df = 300)\n",
    "\n",
    "# TF-IDF 벡터화 학습\n",
    "tfidf_vect.fit(X_train)\n",
    "\n",
    "# TF-IDF 벡터로 변환\n",
    "X_train_tfidf_vect2 = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect2 = tfidf_vect.transform(X_test)\n",
    "\n",
    "\n",
    "# LogisticRegression을 이용해 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "## 학습\n",
    "lr_clf.fit(X_train_tfidf_vect2, y_train)\n",
    "\n",
    "## 예측\n",
    "pred2 = lr_clf.predict(X_test_tfidf_vect2)\n",
    "\n",
    "## 평가\n",
    "accuracy2 = accuracy_score(y_test, pred2)\n",
    "\n",
    "print(f'파라미터를 다르게 한 TF-IDF Logisitc Regression의 예측 정확도는 {np.round(accuracy2,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47256121",
   "metadata": {},
   "source": [
    "#### GridSearchCV를 이용한 하이퍼파라미터 최적화 수행 후 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f95fbc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Logistic Regression best C parameter: {'C': 10}\n",
      "TF-IDF Vectorized Logistic Regression의 예측 정확도: 0.685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# GridSearchCV로 학습/평가/예측: 최적 C,값 도출 튜닝 수행 CV = 3인 폴드 세트로 설정\n",
    "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid = params, cv = 3, scoring = 'accuracy', verbose = 1)\n",
    "\n",
    "\n",
    "## 학습\n",
    "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
    "print(f'Logistic Regression best C parameter: {grid_cv_lr.best_params_}')\n",
    "\n",
    "## 예측\n",
    "preds = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f'TF-IDF Vectorized Logistic Regression의 예측 정확도: {np.round(accuracy, 3)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28a162",
   "metadata": {},
   "source": [
    "# 감성 분석\n",
    "- 문서의 주관적인 감성/의견/감정/기분 등을 파악하기 위한 방법\n",
    "- 소셜미디어, 여론조사, 온라인리뷰, 피드백 등 다양한 분야에서 활용되고 있음\n",
    "- 문서 내 텍스트가 나타내는 여러가지 주관적인 단어와 문맥을 기반으로 감성(Sentiment) 수치를 계산하는 방법 이용\n",
    "- 지도학습/비지도학습 방식으로 나뉨\n",
    "    - 지도학습: 학습 데이터와 타깃 레이블 값을 기반으로 감성 분석 학습 수행 후 다른 데이터의 감성 분석 예측\n",
    "    - 비지도학습: 'Lexicon'이라는 일종의 감성 어휘 사전 이용, 용어와 문맥에 대한 다양한 정보르 ㄹ가지고 있으며, 이를 이용해 문서의 긍정/부정 여부 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4b58e",
   "metadata": {},
   "source": [
    "## 실습: IMDB 영화평"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f97fb8",
   "metadata": {},
   "source": [
    "## 지도학습 기반 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f83ae285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_df = pd.read_csv('./labeledTrainData.tsv', header = 0, sep = '\\t', quoting = 3)\n",
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc5ab0",
   "metadata": {},
   "source": [
    "- id: 각 데이터의 id\n",
    "- sentiment: 영화평 label값\n",
    "- review: 영화평 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "16b58253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb285ba2",
   "metadata": {},
   "source": [
    "### 정규표현식으로 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "327ce419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#<br /> 태그 공백으로 변환\n",
    "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
    "\n",
    "# 영어 문자열이 아닌 것은 모두 공백으로 변환\n",
    "review_df['review'] = review_df['review'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0f11aa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay   Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him   The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music   Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene   Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  '"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "review_df['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa43b91",
   "metadata": {},
   "source": [
    "### 학습용/테트스용 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b932e",
   "metadata": {},
   "source": [
    "#### 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94cb57e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500,), (7500,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df['review']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df, test_size = 0.3, random_state = 156)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3147c3c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3724     ThisversionmovedalittleslowformytasteandIsuppo...\n",
       "23599    IreallyenjoyedthisfilmbecauseIhaveatremendousi...\n",
       "11331    Sawthisinthetheaterinandfelloutofmychairlaughi...\n",
       "15745    RecentlyIwaslookingforthenewlyissuedWideScreen...\n",
       "845      Escapingthelifeofbeingpimpedbyherfatherandthes...\n",
       "                               ...                        \n",
       "6955     Thisisagenerallynicefilmwithgoodstorygreatacto...\n",
       "7653     TherealshameofTheGatheringisnotinthebadactingn...\n",
       "9634     Inwhatcouldhavebeenanotherwiserunofthemillmedi...\n",
       "6860     ExcellentPOWadventureadaptedbyEricWilliamsfrom...\n",
       "24108    ThisonefeaturesallthebadeffectofPriorscheapomo...\n",
       "Name: review, Length: 17500, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80a473a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1692     MygirlfriendandIwerestunnedbyhowbadthisfilmwas...\n",
       "13392    Whatdoyouexpectwhenthereisnoscripttobeginwitha...\n",
       "21063    ThisisaGermanfilmfromthatissomethingtodowithso...\n",
       "10335    RichardTylerisalittleboywhoisscaredofeverythin...\n",
       "16847    IrunagrouptostopcomedianexploitationandIjustsp...\n",
       "                               ...                        \n",
       "14848    Ifyouliketocommentonfilmswherethescriptarriveh...\n",
       "8450     FirstletmesaythatNotoriousisanabsolutelycharmi...\n",
       "8221     Realisticmoviesureexceptforthefactthatthechara...\n",
       "10638    IwillspendafewdaysdedicatedtoRonHowardbeforeIs...\n",
       "20673    JerryspiesTomlisteningtoacreepystoryontheradio...\n",
       "Name: review, Length: 7500, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612f458",
   "metadata": {},
   "source": [
    "#### 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "547727de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df.drop(['id','sentiment'], axis = 1, inplace = False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df, test_size = 0.3, random_state = 156)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6f55f7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>This version moved a little slow for my taste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23599</th>\n",
       "      <td>I really enjoyed this film because I have a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11331</th>\n",
       "      <td>Saw this in the theater in     and fell out o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15745</th>\n",
       "      <td>Recently I was looking for the newly issued W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>Escaping the life of being pimped by her fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6955</th>\n",
       "      <td>This is a generally nice film  with good stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7653</th>\n",
       "      <td>The real shame of   The Gathering   is not in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9634</th>\n",
       "      <td>In what could have been an otherwise run of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>Excellent P O W  adventure  adapted by Eric W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24108</th>\n",
       "      <td>This one features all the  bad  effect of Pri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review\n",
       "3724    This version moved a little slow for my taste...\n",
       "23599   I really enjoyed this film because I have a t...\n",
       "11331   Saw this in the theater in     and fell out o...\n",
       "15745   Recently I was looking for the newly issued W...\n",
       "845     Escaping the life of being pimped by her fath...\n",
       "...                                                  ...\n",
       "6955    This is a generally nice film  with good stor...\n",
       "7653    The real shame of   The Gathering   is not in...\n",
       "9634    In what could have been an otherwise run of t...\n",
       "6860    Excellent P O W  adventure  adapted by Eric W...\n",
       "24108   This one features all the  bad  effect of Pri...\n",
       "\n",
       "[17500 rows x 1 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "073e9f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>My girlfriend and I were stunned by how bad t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13392</th>\n",
       "      <td>What do you expect when there is no script to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21063</th>\n",
       "      <td>This is a German film from      that is somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335</th>\n",
       "      <td>Richard Tyler is a little boy who is scared o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16847</th>\n",
       "      <td>I run a group to stop comedian exploitation a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14848</th>\n",
       "      <td>If you like to comment on films where the scr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8450</th>\n",
       "      <td>First  let me say that Notorious is an absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8221</th>\n",
       "      <td>Realistic movie sure except for the fact that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10638</th>\n",
       "      <td>I will spend a few days dedicated to Ron Howa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20673</th>\n",
       "      <td>Jerry spies Tom listening to a creepy story o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review\n",
       "1692    My girlfriend and I were stunned by how bad t...\n",
       "13392   What do you expect when there is no script to...\n",
       "21063   This is a German film from      that is somet...\n",
       "10335   Richard Tyler is a little boy who is scared o...\n",
       "16847   I run a group to stop comedian exploitation a...\n",
       "...                                                  ...\n",
       "14848   If you like to comment on films where the scr...\n",
       "8450    First  let me say that Notorious is an absolu...\n",
       "8221    Realistic movie sure except for the fact that...\n",
       "10638   I will spend a few days dedicated to Ron Howa...\n",
       "20673   Jerry spies Tom listening to a creepy story o...\n",
       "\n",
       "[7500 rows x 1 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2f5c4",
   "metadata": {},
   "source": [
    "### 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeff15d",
   "metadata": {},
   "source": [
    "#### CounterVectorizer 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "901b2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.886, ROC-AUC는 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "# CounterVectorizer 파라미터 지정: 스톱워드는 english, ngram_range는 (1,2) \n",
    "# LogiticRegression의 C는 10으로 설정\n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words = 'english', ngram_range = (1,2))),\n",
    "    ('lr_clf', LogisticRegression(solver = 'liblinear', C = 10))\n",
    "])\n",
    "\n",
    "# pipeline 객체를 이용해 학습/예측/평가 수행\n",
    "## 학습\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "\n",
    "## 예측\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[: ,1]\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "roc_auc_score = roc_auc_score(y_test, pred_probs)\n",
    "\n",
    "print(f'예측 정확도는 {np.round(accuracy,3)}, ROC-AUC는 {np.round(roc_auc_score,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb836e70",
   "metadata": {},
   "source": [
    "#### TF-IDF 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f06ae029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8936, ROC-AUC는 0.959800534941953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# TfidfVectorizer 파라미터 지정: 스톱워드는 english, ngram_range는 (1,2) \n",
    "# LogiticRegression의 C는 10으로 설정\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words = 'english', ngram_range = (1,2))),\n",
    "    ('lr_clf', LogisticRegression(solver = 'liblinear', C = 10))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline 객체를 이용해 학습/예측/평가 수행\n",
    "## 학습\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "\n",
    "## 예측\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "roc_auc = roc_auc_score(y_test, pred_probs)\n",
    "\n",
    "print(f'예측 정확도는 {accuracy}, ROC-AUC는 {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf513e3a",
   "metadata": {},
   "source": [
    "## 비지도학습 기반 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f42ff3",
   "metadata": {},
   "source": [
    "- 비지도 감성분석은 Lexicon을 기반으로 함\n",
    "- 많은 감성 분석용 데이터는 결정된 레이블 값을 가지고 있지 않아서 Lexicon이 유용하게 사용됨\n",
    "- 다만, 한글 지원이 되지 않음\n",
    "- Lexicon은 일반적으로 어휘집을 의미하지만, 주로 감성만을 분석하기 위해 지원하는 감성 어휘사전\n",
    "    - 감성 어휘사전은 긍정감성/부정감성의 정도를 의미하는 감성 지수를 가짐\n",
    "    - 감성 지수는 단어의 위치나 주변 단어, 문맥, POS(품사) 등을 참고해 결정됨\n",
    "    - NLTK 패키지에서 포함된 모듈임\n",
    "\n",
    "\n",
    "- NLP 패키지\n",
    "    - WordNet: 단순한 어휘사전이 아닌 시맨틱 분석을 제공하는 어휘사전임\n",
    "        - 시맨틱(Semantic): 문맥상의 의미\n",
    "        - 다양한 상황에서 같은 어휘라도 다르게 사용되는 어휘의 시맨틱 정보를 제공하며, 이를 위해 각각의 품사로 구성된 개별 단어를 Synset이라는 개념을 이용해 표현함\n",
    "        - synset은 단순한 하나의 단어가 아니라 그 단어가 가지는 문맥, 시맨틱 정보를 제공함\n",
    "    - SentiWordNet: NLTK패키지의 WordNet과 유사하게 감성 단어 전용의 WordNet을 구현한 것\n",
    "        - WordNet의 synset 개념을 감성 분석에 적용한 것\n",
    "        - WordNet의 synset 별로 긍정/부정/객관성 3가지 감성 점수를 할당함\n",
    "    - VADER: 주로 소셜 미디어의 텍스트에 대한 감성 분석을 제공하기 위한 패키지\n",
    "        - 뛰어난 감성 분석 결과를 제공하며, 비교적 빠른 수행시간을 보장해 대용량 텍스트 데이터에 잘 사용되는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f57526a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3da3d",
   "metadata": {},
   "source": [
    "### WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "22fd85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets 반환 type: <class 'list'>\n",
      "synsets 반환 값 개수: 18\n",
      "synsets 반환 값: \n",
      "[Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# 단어 지정\n",
    "term = 'present'\n",
    "\n",
    "# present라는 단어로 wordnet의 synsets 생성\n",
    "synsets = wn.synsets(term)\n",
    "\n",
    "\n",
    "print(f'synsets 반환 type: {type(synsets)}')\n",
    "print(f'synsets 반환 값 개수: {len(synsets)}')\n",
    "print(f'synsets 반환 값: \\n{synsets}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81118362",
   "metadata": {},
   "source": [
    "- synset 객체의 파라미터: 'present.n.01' -> POS태그를 나타냄\n",
    "    - present: 의미\n",
    "    - n: 품사\n",
    "    - 01: 품사로서 가지는 의미가 여러가지가 있어서 이를 구분하는 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61389fe2",
   "metadata": {},
   "source": [
    "- synset객체의 속성:\n",
    "    - synset.lexname() -> POS: 품사 \n",
    "    - synset.definition() -> Definition: 정의\n",
    "    - synset.lemma_names() -> Lemma: 부명제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c73ff2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***synset: present.n.01 ****\n",
      "POS: noun.time\n",
      "Definition: the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
      "Lemmas: ['present', 'nowadays']\n",
      "\n",
      "\n",
      "***synset: present.n.02 ****\n",
      "POS: noun.possession\n",
      "Definition: something presented as a gift\n",
      "Lemmas: ['present']\n",
      "\n",
      "\n",
      "***synset: present.n.03 ****\n",
      "POS: noun.communication\n",
      "Definition: a verb tense that expresses actions or states at the time of speaking\n",
      "Lemmas: ['present', 'present_tense']\n",
      "\n",
      "\n",
      "***synset: show.v.01 ****\n",
      "POS: verb.perception\n",
      "Definition: give an exhibition of to an interested audience\n",
      "Lemmas: ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "\n",
      "\n",
      "***synset: present.v.02 ****\n",
      "POS: verb.communication\n",
      "Definition: bring forward and present to the mind\n",
      "Lemmas: ['present', 'represent', 'lay_out']\n",
      "\n",
      "\n",
      "***synset: stage.v.01 ****\n",
      "POS: verb.creation\n",
      "Definition: perform (a play), especially on a stage\n",
      "Lemmas: ['stage', 'present', 'represent']\n",
      "\n",
      "\n",
      "***synset: present.v.04 ****\n",
      "POS: verb.possession\n",
      "Definition: hand over formally\n",
      "Lemmas: ['present', 'submit']\n",
      "\n",
      "\n",
      "***synset: present.v.05 ****\n",
      "POS: verb.stative\n",
      "Definition: introduce\n",
      "Lemmas: ['present', 'pose']\n",
      "\n",
      "\n",
      "***synset: award.v.01 ****\n",
      "POS: verb.possession\n",
      "Definition: give, especially as an honor or reward\n",
      "Lemmas: ['award', 'present']\n",
      "\n",
      "\n",
      "***synset: give.v.08 ****\n",
      "POS: verb.possession\n",
      "Definition: give as a present; make a gift of\n",
      "Lemmas: ['give', 'gift', 'present']\n",
      "\n",
      "\n",
      "***synset: deliver.v.01 ****\n",
      "POS: verb.communication\n",
      "Definition: deliver (a speech, oration, or idea)\n",
      "Lemmas: ['deliver', 'present']\n",
      "\n",
      "\n",
      "***synset: introduce.v.01 ****\n",
      "POS: verb.communication\n",
      "Definition: cause to come to know personally\n",
      "Lemmas: ['introduce', 'present', 'acquaint']\n",
      "\n",
      "\n",
      "***synset: portray.v.04 ****\n",
      "POS: verb.creation\n",
      "Definition: represent abstractly, for example in a painting, drawing, or sculpture\n",
      "Lemmas: ['portray', 'present']\n",
      "\n",
      "\n",
      "***synset: confront.v.03 ****\n",
      "POS: verb.communication\n",
      "Definition: present somebody with something, usually to accuse or criticize\n",
      "Lemmas: ['confront', 'face', 'present']\n",
      "\n",
      "\n",
      "***synset: present.v.12 ****\n",
      "POS: verb.communication\n",
      "Definition: formally present a debutante, a representative of a country, etc.\n",
      "Lemmas: ['present']\n",
      "\n",
      "\n",
      "***synset: salute.v.06 ****\n",
      "POS: verb.communication\n",
      "Definition: recognize with a gesture prescribed by a military regulation; assume a prescribed position\n",
      "Lemmas: ['salute', 'present']\n",
      "\n",
      "\n",
      "***synset: present.a.01 ****\n",
      "POS: adj.all\n",
      "Definition: temporal sense; intermediate between past and future; now existing or happening or in consideration\n",
      "Lemmas: ['present']\n",
      "\n",
      "\n",
      "***synset: present.a.02 ****\n",
      "POS: adj.all\n",
      "Definition: being or existing in a specified place\n",
      "Lemmas: ['present']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print(f'***synset: {synset.name()} ****')\n",
    "    print(f'POS: {synset.lexname()}')\n",
    "    print(f'Definition: {synset.definition()}')\n",
    "    print(f'Lemmas: {synset.lemma_names()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4da53",
   "metadata": {},
   "source": [
    "#### path_similarity 메서드: 어휘 간 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "06599038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tree  lion  tiger   cat   dog\n",
       "tree   1.00  0.07   0.14  0.08  0.12\n",
       "lion   0.07  1.00   0.08  0.25  0.17\n",
       "tiger  0.14  0.08   1.00  0.09  0.17\n",
       "cat    0.08  0.25   0.09  1.00  0.20\n",
       "dog    0.12  0.17   0.17  0.20  1.00"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# synset 객체를 단어별로 생성\n",
    "\n",
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog = wn.synset('dog.n.01')\n",
    "\n",
    "\n",
    "# 단어 객체 리스트\n",
    "entities = [tree, lion, tiger, cat, dog]\n",
    "\n",
    "# 유사도 측정값 담을 빈 리스트\n",
    "similarities = []\n",
    "\n",
    "# 단어 객체 이름\n",
    "entity_names = [entity.name().split('.')[0] for entity in entities]\n",
    "\n",
    "# 단어별 synset을 반복하면서 다른 단어와의 synset유사도 측정\n",
    "for entity in entities:\n",
    "    similarity = [round(entity.path_similarity(compared_entity),2)\n",
    "                     for compared_entity in entities ]\n",
    "    similarities.append(similarity)\n",
    "    \n",
    "# 개별 단어별 synset과 다른 단어의 synset과의 유사도를 데이터프레임형태로 생성\n",
    "similarity_df = pd.DataFrame(similarities, columns = entity_names, index = entity_names)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6dc1fa",
   "metadata": {},
   "source": [
    "### SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "senti_synsets = list(swn.senti_synsets('slow'))\n",
    "print('senti_synsets()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e08aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc6a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56510e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
