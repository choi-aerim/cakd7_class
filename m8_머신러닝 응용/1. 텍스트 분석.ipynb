{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33de5827",
   "metadata": {},
   "source": [
    "# 텍스트분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc06bf",
   "metadata": {},
   "source": [
    "## 클렌징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44058de",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfa392",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ceed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp38-cp38-win_amd64.whl (267 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: click in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.9.13 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6197c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# 마침표, 개행문자 등 데이터 세트 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sent_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 문장 개수:3\n",
      "\n",
      " 출력: \n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "\n",
    "print(f' sent_tokenize 객체 타입: \\n{type(sentences)},\\n\\n 문장 개수:{len(sentences)}\\n')\n",
    "print(f' 출력: \\n{sentences}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab04f76",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "- 문장을 단어로 토큰화\n",
    "- 공백, 쉼포, 마침표, 개행문자 등으로 단어를 분리 혹은 정규표현식을 이용해 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de0b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 단어 개수:15\n",
      "\n",
      " 출력: \n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = sentences[0]\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(f' word_tokenize 객체 타입: \\n{type(words)},\\n\\n 단어 개수:{len(words)}\\n')\n",
    "print(f' 출력: \\n{words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e5355",
   "metadata": {},
   "source": [
    "### 문장 및 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "794a5fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 단어 개수:3\n",
      "\n",
      " 출력:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Matrix',\n",
       "  'is',\n",
       "  'everywhere',\n",
       "  'its',\n",
       "  'all',\n",
       "  'around',\n",
       "  'us',\n",
       "  ',',\n",
       "  'here',\n",
       "  'even',\n",
       "  'in',\n",
       "  'this',\n",
       "  'room',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'can',\n",
       "  'see',\n",
       "  'it',\n",
       "  'out',\n",
       "  'your',\n",
       "  'window',\n",
       "  'or',\n",
       "  'on',\n",
       "  'your',\n",
       "  'television',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'feel',\n",
       "  'it',\n",
       "  'when',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  'work',\n",
       "  ',',\n",
       "  'or',\n",
       "  'go',\n",
       "  'to',\n",
       "  'church',\n",
       "  'or',\n",
       "  'pay',\n",
       "  'your',\n",
       "  'taxes',\n",
       "  '.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하도록 만드는 사용자 함수 생성\n",
    "def tokenize_text(text):\n",
    "    ## 문장 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    ## 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "# 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(f' word_tokenize 객체 타입: \\n{type(word_tokens)},\\n\\n 단어 개수:{len(word_tokens)}\\n')\n",
    "print(f' 출력:')\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a4453",
   "metadata": {},
   "source": [
    "### n_gram\n",
    "- 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec783b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "916081bf",
   "metadata": {},
   "source": [
    "## 스톱워드 제거\n",
    "- 문장을 구성하는 필수 문법 요소지만 문맥적으로 큰 의미가 없는 단어를 사전에 제거\n",
    "- 빈번하게 텍스트에 나타나면 중요 단어로 인지될 수 있기 때문에 중요한 작업임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bb89b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 스톱워드 목록 다운로드\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19c5ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스톱워드 리스트 내 단어 개수:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('스톱워드 리스트 내 단어 개수:')\n",
    "len(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9597555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스톱워드 리스트:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('스톱워드 리스트:')\n",
    "nltk.corpus.stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89a63db1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'],\n",
       " ['see', 'window', 'television', '.'],\n",
       " ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 스톱워드 리스트 생성\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    \n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        # 소문자로 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱워드 리스트에 포함되지 않으면 리스트에 담기\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "        # 문장별 걸러진 단어를 최종 리스트에 담기    \n",
    "    all_tokens.append(filtered_words)\n",
    "        \n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0af948",
   "metadata": {},
   "source": [
    "## Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d6157",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- 원형 단어로 변환 시, 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
    "- 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098538f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant faciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Stemmer 객체 생성\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "# stem('단어') 메서드를 활용해 원하는 단어의 stemming 수행\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('faciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff336485",
   "metadata": {},
   "source": [
    "- work의 경우 working, works, worked 등 기본 단어에 ing, s, ed가 붙는 단순 형태라서 제대로 인식함\n",
    "- 하지만, 다른 단의 경우 비교형, 최상급형에서 원형을 정확히 찾지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5f282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab9b8f45",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- 품사와 같은 문법적 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아줌\n",
    "- 따라서 수행시간이 Stemming보다 더 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731b6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607da5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fccec003",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6ccb3",
   "metadata": {},
   "source": [
    "## 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc73c8a",
   "metadata": {},
   "source": [
    "### BOW벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6d36b",
   "metadata": {},
   "source": [
    "### 희소행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c76829",
   "metadata": {},
   "source": [
    "#### 희소행렬 형태의 BOW벡터화한 데이터프레임의 예시를 배열로 만들어서 COO방식으로 저장되는 것 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16425bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 예시 배열 만들기\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da47f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# data의 행의 위치와 열의 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 COO형식 희소행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45006ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66caf5e5",
   "metadata": {},
   "source": [
    "### 희소행렬 - CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19cf443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2 =  np.array([[0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0],[2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32198f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]] \n",
      "\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray(), '\\n')\n",
    "\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5545d5c",
   "metadata": {},
   "source": [
    "#### 과제1: 10행 10열의 희소행렬 (0의 비중 70% 이상)을 생성한 후 COO, CSR 방식으로 변환 후 다시 희소 행렬 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c1526ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1, 0, 2, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a6dac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 1 0 2 0]]\n",
      "[3 0 1 0 2 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(d)\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mindex(d))\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "x = dense.reshape(1,-1)\n",
    "print(x)\n",
    "for d in x:\n",
    "    print(d)\n",
    "    if d != 0:\n",
    "        print(x.index(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(dense):\n",
    "    one_dense = dense.reshape(1,-1)\n",
    "    data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe4f15",
   "metadata": {},
   "source": [
    "## 실습: 텍스트 분류- 20 뉴스 그룹 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d83dd29",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: 'C:\\\\Users\\\\NEW\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_20newsgroups\n\u001b[1;32m----> 3\u001b[0m news_data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m156\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(news_data\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cakd7\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:269\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    268\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m     cache \u001b[38;5;241m=\u001b[39m \u001b[43m_download_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwenty_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_path\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20Newsgroups dataset not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cakd7\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:78\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     76\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecompressing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, archive_path)\n\u001b[0;32m     77\u001b[0m tarfile\u001b[38;5;241m.\u001b[39mopen(archive_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr:gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mextractall(path\u001b[38;5;241m=\u001b[39mtarget_dir)\n\u001b[1;32m---> 78\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Store a zipped pickle\u001b[39;00m\n\u001b[0;32m     81\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     82\u001b[0m     train\u001b[38;5;241m=\u001b[39mload_files(train_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     83\u001b[0m     test\u001b[38;5;241m=\u001b[39mload_files(test_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     84\u001b[0m )\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: 'C:\\\\Users\\\\NEW\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate.tar.gz'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset = 'all', random_state = 156)\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d64277dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (459988568.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [39]\u001b[1;36m\u001b[0m\n\u001b[1;33m    20news-bydate.tar.gz.close()\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af160112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도\\n', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('\\n')\n",
    "print('target 클래스의 이름들 \\n', news_data.target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
