{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cc06bf",
   "metadata": {},
   "source": [
    "## 클렌징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44058de",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfa392",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ceed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp38-cp38-win_amd64.whl (267 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: click in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\new\\anaconda3\\envs\\cakd7\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.9.13 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6197c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# 마침표, 개행문자 등 데이터 세트 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sent_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 문장 개수:3\n",
      "\n",
      " 출력: \n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "\n",
    "print(f' sent_tokenize 객체 타입: \\n{type(sentences)},\\n\\n 문장 개수:{len(sentences)}\\n')\n",
    "print(f' 출력: \\n{sentences}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab04f76",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "- 문장을 단어로 토큰화\n",
    "- 공백, 쉼포, 마침표, 개행문자 등으로 단어를 분리 혹은 정규표현식을 이용해 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de0b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 단어 개수:15\n",
      "\n",
      " 출력: \n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = sentences[0]\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(f' word_tokenize 객체 타입: \\n{type(words)},\\n\\n 단어 개수:{len(words)}\\n')\n",
    "print(f' 출력: \\n{words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e5355",
   "metadata": {},
   "source": [
    "### 문장 및 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "794a5fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word_tokenize 객체 타입: \n",
      "<class 'list'>,\n",
      "\n",
      " 단어 개수:3\n",
      "\n",
      " 출력:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Matrix',\n",
       "  'is',\n",
       "  'everywhere',\n",
       "  'its',\n",
       "  'all',\n",
       "  'around',\n",
       "  'us',\n",
       "  ',',\n",
       "  'here',\n",
       "  'even',\n",
       "  'in',\n",
       "  'this',\n",
       "  'room',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'can',\n",
       "  'see',\n",
       "  'it',\n",
       "  'out',\n",
       "  'your',\n",
       "  'window',\n",
       "  'or',\n",
       "  'on',\n",
       "  'your',\n",
       "  'television',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'feel',\n",
       "  'it',\n",
       "  'when',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  'work',\n",
       "  ',',\n",
       "  'or',\n",
       "  'go',\n",
       "  'to',\n",
       "  'church',\n",
       "  'or',\n",
       "  'pay',\n",
       "  'your',\n",
       "  'taxes',\n",
       "  '.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하도록 만드는 사용자 함수 생성\n",
    "def tokenize_text(text):\n",
    "    ## 문장 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    ## 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "# 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(f' word_tokenize 객체 타입: \\n{type(word_tokens)},\\n\\n 단어 개수:{len(word_tokens)}\\n')\n",
    "print(f' 출력:')\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a4453",
   "metadata": {},
   "source": [
    "### n_gram\n",
    "- 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916081bf",
   "metadata": {},
   "source": [
    "## 스톱워드 제거\n",
    "- 문장을 구성하는 필수 문법 요소지만 문맥적으로 큰 의미가 없는 단어를 사전에 제거\n",
    "- 빈번하게 텍스트에 나타나면 중요 단어로 인지될 수 있기 때문에 중요한 작업임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bb89b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 스톱워드 목록 다운로드\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19c5ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스톱워드 리스트 내 단어 개수:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('스톱워드 리스트 내 단어 개수:')\n",
    "len(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9597555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스톱워드 리스트:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('스톱워드 리스트:')\n",
    "nltk.corpus.stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89a63db1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'],\n",
       " ['see', 'window', 'television', '.'],\n",
       " ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 스톱워드 리스트 생성\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    \n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        # 소문자로 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱워드 리스트에 포함되지 않으면 리스트에 담기\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "        # 문장별 걸러진 단어를 최종 리스트에 담기    \n",
    "    all_tokens.append(filtered_words)\n",
    "        \n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0af948",
   "metadata": {},
   "source": [
    "## Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d6157",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- 원형 단어로 변환 시, 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
    "- 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098538f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant faciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Stemmer 객체 생성\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "# stem('단어') 메서드를 활용해 원하는 단어의 stemming 수행\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('faciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff336485",
   "metadata": {},
   "source": [
    "- work의 경우 working, works, worked 등 기본 단어에 ing, s, ed가 붙는 단순 형태라서 제대로 인식함\n",
    "- 하지만, 다른 단의 경우 비교형, 최상급형에서 원형을 정확히 찾지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b8f45",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- 품사와 같은 문법적 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아줌\n",
    "- 따라서 수행시간이 Stemming보다 더 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731b6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607da5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NEW\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fccec003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6ccb3",
   "metadata": {},
   "source": [
    "## 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc73c8a",
   "metadata": {},
   "source": [
    "### BOW벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6d36b",
   "metadata": {},
   "source": [
    "### 희소행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c76829",
   "metadata": {},
   "source": [
    "#### 희소행렬 형태의 BOW벡터화한 데이터프레임의 예시를 배열로 만들어서 COO방식으로 저장되는 것 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16425bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 예시 배열 만들기\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da47f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# data의 행의 위치와 열의 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 COO형식 희소행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45006ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66caf5e5",
   "metadata": {},
   "source": [
    "### 희소행렬 - CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19cf443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2 =  np.array([[0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0],[2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32198f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]] \n",
      "\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray(), '\\n')\n",
    "\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5545d5c",
   "metadata": {},
   "source": [
    "#### 과제1: 10행 10열의 희소행렬 (0의 비중 70% 이상)을 생성한 후 COO, CSR 방식으로 변환 후 다시 희소 행렬 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f95f17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 7, 0, 0, 0, 0, 0, 1, 0, 7],\n",
       "       [5, 1, 0, 0, 6, 0, 0, 2, 0, 4],\n",
       "       [0, 0, 0, 6, 9, 0, 0, 0, 0, 2],\n",
       "       [6, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 2, 0, 2, 0, 0, 4, 0],\n",
       "       [0, 0, 8, 0, 0, 2, 7, 0, 0, 0],\n",
       "       [0, 4, 0, 6, 0, 6, 0, 0, 0, 7],\n",
       "       [0, 0, 2, 0, 9, 2, 0, 0, 4, 0],\n",
       "       [0, 0, 2, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 5, 0, 0, 0, 0, 9, 0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random_int_li = np.random.randint(1,10, size = 30).tolist() + np.zeros(70, int).tolist()\n",
    "random.shuffle(random_int_li)\n",
    "\n",
    "sparse_matrix = np.array(random_int_li).reshape(10,10)\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d83b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_idx(sparse_matrix):\n",
    "    data = []\n",
    "    row_pos =[]\n",
    "    col_pos = []\n",
    "\n",
    "    for row_idx, rows in enumerate(sparse_matrix):\n",
    "        for col_idx, i in enumerate(rows):\n",
    "            # 0이 아닌 원소 추출\n",
    "            if i != 0:\n",
    "                data.append(i)\n",
    "                # 0이 아닌 원소의 행, 열 인덱스 추출\n",
    "                row_pos.append(row_idx)\n",
    "                col_pos.append(col_idx)        \n",
    "    row_pos_ind = [0]\n",
    "    cumsum_i = 0\n",
    "    for i in range(10):\n",
    "        cumsum_i += row_pos.count(i)-1\n",
    "        row_pos_ind.append(cumsum_i)\n",
    "    \n",
    "    return data, row_pos, col_pos, row_pos_ind\n",
    "\n",
    "data, row_pos, col_pos, row_pos_ind = sparse_idx(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc4c4f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 7 0 0 0 0 0 1 0 7]\n",
      " [5 1 0 0 6 0 0 2 0 4]\n",
      " [0 0 0 6 9 0 0 0 0 2]\n",
      " [6 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 2 0 0 4 0]\n",
      " [0 0 8 0 0 2 7 0 0 0]\n",
      " [0 4 0 6 0 6 0 0 0 7]\n",
      " [0 0 2 0 9 2 0 0 4 0]\n",
      " [0 0 2 0 0 0 0 0 1 0]\n",
      " [0 0 0 5 0 0 0 0 9 0]] \n",
      "\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 7 0 0 0 0 0 1 0 0]\n",
      " [5 1 0 0 6 0 0 0 0 7]\n",
      " [0 0 0 0 0 0 0 2 0 4]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 6 9 0 0 0 0 0]\n",
      " [6 0 0 0 0 0 0 0 0 2]\n",
      " [0 0 0 2 0 2 0 0 4 0]\n",
      " [0 0 8 0 0 2 7 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 6 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "## COO\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "sparse_coo.toarray()\n",
    "\n",
    "## CSR\n",
    "sparse_csr = sparse.csr_matrix((data, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray(), '\\n')\n",
    "\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe4f15",
   "metadata": {},
   "source": [
    "# 실습: 텍스트 분류- 20 뉴스 그룹 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d83dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset = 'all', random_state = 156)\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af160112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도\n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "target 클래스의 이름들 \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도\\n', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('\\n')\n",
    "print('target 클래스의 이름들 \\n', news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a32ff5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c7954e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: 11314, 테스트 데이터 크기: 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset = 'train'으로 학습용 데이터만 추출, remove = ('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news = fetch_20newsgroups(subset = 'train',\n",
    "                                remove = ('headers', 'footers', 'quotes'),\n",
    "                                random_state = 156)\n",
    "\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "\n",
    "# subset = 'train'으로 학습용 데이터만 추출, remove = ('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news = fetch_20newsgroups(subset = 'test',\n",
    "                               remove = ('headers', 'footers', 'quotes'),\n",
    "                               random_state = 156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "\n",
    "print(f'학습 데이터 크기: {len(train_news.data)}, 테스트 데이터 크기: {len(test_news.data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad29d4",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀모델 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7181fd3",
   "metadata": {},
   "source": [
    "### CountVectorizer 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "90934653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 텍스트의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorization으로 피처 벡터화 변환 수행\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "\n",
    "# 학습 데이터로 fit() 된 CountVectorizer를 이용해 테스트 데이터를 피처 벡처화 변환 수행\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "\n",
    "print('학습 데이터 텍스트의 CountVectorizer Shape:', X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "459a4156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "## 학습\n",
    "### 주의: 벡터화 한 x_train 데이터 세트 이용해야 함\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "\n",
    "## 예측\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(f'CountVectorized Logistic Regression의 예측 정확도는 {np.round(accuracy,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2497d",
   "metadata": {},
   "source": [
    "### TF-IDF 기반"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951b65",
   "metadata": {},
   "source": [
    "#### 기본 파라미터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d6132d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터화를 적용해 학습데이터 테스트 데이터세트 변환\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF 벡터화를 적용#### 파라미터 수정 후 TF-IDF기반 로지스틱 회귀 모델 적용\n",
    "tfidf_vect.fit(X_train)\n",
    "\n",
    "#X_train, X_test 데이터를 TF-IDF 벡터로 변형시키기\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "\n",
    "# LogisticRegression을 이용해 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "## 학습\n",
    "### 주의: 벡터화한 x_train 데이터 세트 이용\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "\n",
    "## 예측\n",
    "lr_clf.predict(X_test_tfidf_vect)\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(f'TF-IDF Logistic Regression의 예측 정확도는 {np.round(accuracy,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f683e",
   "metadata": {},
   "source": [
    "#### 파라미터 수정 후 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0e84fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파라미터를 다르게 한 TF-IDF Logisitc Regression의 예측 정확도는 0.69\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 수정한 TF-IDF 객체 생성\n",
    "tfidf_vect = TfidfVectorizer(stop_words = 'english', \n",
    "                             ngram_range = (1,2),\n",
    "                             max_df = 300)\n",
    "\n",
    "# TF-IDF 벡터화 학습\n",
    "tfidf_vect.fit(X_train)\n",
    "\n",
    "# TF-IDF 벡터로 변환\n",
    "X_train_tfidf_vect2 = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect2 = tfidf_vect.transform(X_test)\n",
    "\n",
    "\n",
    "# LogisticRegression을 이용해 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "## 학습\n",
    "lr_clf.fit(X_train_tfidf_vect2, y_train)\n",
    "\n",
    "## 예측\n",
    "pred2 = lr_clf.predict(X_test_tfidf_vect2)\n",
    "\n",
    "## 평가\n",
    "accuracy2 = accuracy_score(y_test, pred2)\n",
    "\n",
    "print(f'파라미터를 다르게 한 TF-IDF Logisitc Regression의 예측 정확도는 {np.round(accuracy2,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47256121",
   "metadata": {},
   "source": [
    "#### GridSearchCV를 이용한 하이퍼파라미터 최적화 수행 후 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f95fbc05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Logistic Regression best C parameter: {'C': 10}\n",
      "TF-IDF Vectorized Logistic Regression의 예측 정확도: 0.685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# GridSearchCV로 학습/평가/예측: 최적 C,값 도출 튜닝 수행 CV = 3인 폴드 세트로 설정\n",
    "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid = params, cv = 3, scoring = 'accuracy', verbose = 1)\n",
    "\n",
    "\n",
    "## 학습\n",
    "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
    "print(f'Logistic Regression best C parameter: {grid_cv_lr.best_params_}')\n",
    "\n",
    "## 예측\n",
    "preds = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "\n",
    "## 평가\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f'TF-IDF Vectorized Logistic Regression의 예측 정확도: {np.round(accuracy, 3)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85a54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
